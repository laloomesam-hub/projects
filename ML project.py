# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ii724wEYlDvJcN60kFHjIQsjeyl72xp5
"""

#importing data frame using pandas
import pandas as pd
#statsmodel.api for using stats functions
import statsmodels.api as sm
#numpy for working with multidimensional data
import numpy as np
#seaborn and matplotlib for visualisation
import seaborn as sns
import matplotlib.pyplot as plt
df = pd.read_csv('/content/Crop_recommendation.csv')
df

#shows statistical summary
print(df.describe())

"""# New Section"""

#for printing histograms of the columns
df['K'].plot(kind='hist',bins=20,title='Potassium',color='purple')

df['N'].plot(kind='hist',bins=20,title='Nitrogen',color='purple')

df['P'].plot(kind='hist',bins=20,title='Phosphorus',color='purple')

df['temperature'].plot(kind='hist',bins=20,title='temperature',color='purple')

df['humidity'].plot(kind='hist',bins=20,title='humidity',color='purple')

df['ph'].plot(kind='hist',bins=20,title='ph',color='purple')

df['rainfall'].plot(kind='hist',bins=20,title='rainfall',color='purple')

print(df.isnull().sum())

#for printing the correlation matrix
numeric_df = df.select_dtypes(include=['number'])
corr_matrix = numeric_df.corr()

fig, ax = plt.subplots()
im = ax.matshow(corr_matrix)

ax.set_xticks(range(len(numeric_df.columns)))
ax.set_xticklabels(numeric_df.columns, rotation=90)
ax.set_yticks(range(len(numeric_df.columns)))
ax.set_yticklabels(numeric_df.columns)
fig.colorbar(im)
plt.show()

#statsmodels.api used for estimating different stats models
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

numeric_df_with_const = sm.add_constant(numeric_df)
vif_data=pd.DataFrame()
vif_data["Variable"]=numeric_df_with_const.columns
vif_data["VIF"] = [variance_inflation_factor(numeric_df_with_const.values,i) for i in range(len(df.columns))]
print(vif_data)
#if VIF is more than 3, there is multi-collinearity in the dataset

#printing box plots for different crops and their corresponding columns
for crop in df['label'].unique():
    crop_df = df[df['label'] == crop]
    plt.figure(figsize=(10, 6))
    sns.boxplot(data=crop_df.drop('label', axis=1))
    plt.title(f'Box Plot of Features for {crop}')
    plt.show()

#to split data into training and testing data
from sklearn.model_selection import train_test_split
#drops label column i.e. names of crops
X = df.drop('label', axis=1)
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

X_train

#decision tree for accuracy
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
dt_classifier=DecisionTreeClassifier()
dt_classifier.fit(X_train,y_train)
y_pred=dt_classifier.predict(X_test)
accuracy=accuracy_score(y_test,y_pred)
print(f"accuracy:{accuracy}")

# random forest
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

#the higher the value of the n-estimator, the better is the model's performance
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

#training the classifier
rf_classifier.fit(X_train, y_train)

# Making predictions on the test set
y_pred_rf = rf_classifier.predict(X_test)

#checking the accuracy
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"Random Forest Accuracy: {accuracy_rf}")

#hyperparameterizing the dataset

import pandas as pd
from sklearn.model_selection import train_test_split
#StandardScaler used for feature scaling
from sklearn.preprocessing import StandardScaler
#used for logistic regression
from sklearn.linear_model import LogisticRegression
#used for hyperparameter tuning
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
#used to encode categorical features into numerical values
from sklearn.preprocessing import LabelEncoder



X = df[['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']]
y = df['label']

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardizing the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Creating a logistic regression model
model = LogisticRegression()


param_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']}


grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')

# Fitting the model to the training data
grid_search.fit(X_train, y_train)


print("Best Parameters:", grid_search.best_params_)

best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# applying Principal Component Analysis

import pandas as pd
from sklearn.preprocessing import StandardScaler
#to perform PCA for dimensionality reduction
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
#to use random forest
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder


X = df[['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']]
y = df['label']

#  dimensionality reduction
pca = PCA(n_components=0.95)
principalComponents = pca.fit_transform(X)

principalDf = pd.DataFrame(data=principalComponents)

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(
    principalDf, y, test_size=0.2, random_state=42
)

rf_model = RandomForestClassifier(random_state=42)

rf_model.fit(X_train, y_train)

y_pred = rf_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy with PCA:", accuracy)